name: Firefly III Test Suite

on:
  pull_request:
    branches:
      - main
      - intgrate_with_chatgpt
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - api
          - ai
          - webhook
          - business
          - security
          - load
          - quality
      environment:
        description: 'Environment to test against'
        required: true
        default: 'staging'
        type: choice
        options:
          - development
          - staging
          - production

# Prevent concurrent GitHub Pages deployments
concurrency:
  group: pages-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

env:
  FIREFLY_BASE_URL: http://52.212.42.101:8080/api/v1
  FIREFLY_URL: http://52.212.42.101:8080
  FIREFLY_VERSION: fireflyiii/core:version-6.3.2
  FIREFLY_DB: mariaDB:noble
  FIREFLY_IMPORTER: fireflyiii/importer:version-1.7.10

permissions:
  contents: write
  issues: write
  pull-requests: write
  pages: write
  id-token: write
  
jobs:
  unit-tests:
    name: Unit Tests (AI + Webhook)
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Unit Tests (AI + Webhook)
        env:
          GITHUB_ACTIONS: true
          API_TESTING_TOKEN: ${{ secrets.API_TESTING_TOKEN }}
          STATIC_CRON_TOKEN: ${{ secrets.STATIC_CRON_TOKEN }}
        run: |
          python run_github_tests.py unit

      - name: Upload Unit Test Allure Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: allure-results-unit-${{ github.run_id }}
          path: allure-results/
          retention-days: 30

  api-tests:
    name: API Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    if: ${{ !cancelled() }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Check Remote Firefly III Availability
        id: firefly_check
        run: |
          if curl -f -s --max-time 10 "${{ env.FIREFLY_URL }}/api/v1/about" > /dev/null; then
            echo "available=true" >> $GITHUB_OUTPUT
            echo "âœ… Remote Firefly III instance is available"
          else
            echo "available=false" >> $GITHUB_OUTPUT
            echo "âš ï¸ Remote Firefly III instance is not available"
          fi

      - name: Run API Tests (if available)
        if: steps.firefly_check.outputs.available == 'true'
        env:
          GITHUB_ACTIONS: true
          FIREFLY_BASE_URL: ${{ env.FIREFLY_BASE_URL }}
          API_TESTING_TOKEN: ${{ secrets.API_TESTING_TOKEN }}
          STATIC_CRON_TOKEN: ${{ secrets.STATIC_CRON_TOKEN }}
        run: |
          python run_github_tests.py api

      - name: Run All Tests (fallback if API unavailable)
        if: steps.firefly_check.outputs.available == 'false'
        env:
          GITHUB_ACTIONS: true
          API_TESTING_TOKEN: ${{ secrets.API_TESTING_TOKEN }}
          STATIC_CRON_TOKEN: ${{ secrets.STATIC_CRON_TOKEN }}
        run: |
          echo "ğŸ”„ Remote Firefly III unavailable, running unit tests only"
          python run_github_tests.py unit

      - name: Upload API Test Allure Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: allure-results-api-${{ github.run_id }}
          path: allure-results/
          retention-days: 30

  comprehensive-tests:
    name: All Tests (Manual Trigger)
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.test_type == 'all'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Comprehensive Test Suite
        env:
          GITHUB_ACTIONS: true
          FIREFLY_BASE_URL: ${{ env.FIREFLY_BASE_URL }}
          API_TESTING_TOKEN: ${{ secrets.API_TESTING_TOKEN }}
          STATIC_CRON_TOKEN: ${{ secrets.STATIC_CRON_TOKEN }}
        run: |
          python run_github_tests.py ${{ github.event.inputs.test_type }}

      - name: Upload Comprehensive Test Allure Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: allure-results-comprehensive-${{ github.run_id }}
          path: allure-results/
          retention-days: 30

  code-quality:
    name: Code Quality & Linting
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install quality tools
        run: |
          python -m pip install --upgrade pip
          pip install flake8 black isort mypy bandit safety pylint
          pip install -r requirements.txt

      - name: Run Black (Code Formatting)
        run: |
          black --check --diff tests/ *.py || echo "::warning::Code formatting issues found"

      - name: Run isort (Import Sorting)
        run: |
          isort --check-only --diff tests/ *.py || echo "::warning::Import sorting issues found"

      - name: Run flake8 (Style Guide)
        run: |
          flake8 tests/ *.py --max-line-length=100 --ignore=E203,W503 || echo "::warning::Style guide violations found"

      - name: Run mypy (Type Checking)
        run: |
          mypy tests/ --ignore-missing-imports || echo "::warning::Type checking issues found"

      - name: Run pylint (Code Analysis)
        run: |
          pylint tests/ *.py --disable=C0114,C0115,C0116,R0903 --exit-zero

      - name: Generate Code Quality Report
        run: |
          mkdir -p quality-reports
          echo "# Code Quality Report" > quality-reports/quality-summary.md
          echo "Generated: $(date)" >> quality-reports/quality-summary.md
          echo "" >> quality-reports/quality-summary.md
          
          # Black check
          if black --check tests/ *.py; then
            echo "âœ… Black: Code formatting is correct" >> quality-reports/quality-summary.md
          else
            echo "âŒ Black: Code formatting issues found" >> quality-reports/quality-summary.md
          fi
          
          # Flake8 check
          if flake8 tests/ *.py --max-line-length=100 --ignore=E203,W503; then
            echo "âœ… Flake8: No style guide violations" >> quality-reports/quality-summary.md
          else
            echo "âŒ Flake8: Style guide violations found" >> quality-reports/quality-summary.md
          fi

      - name: Upload Quality Reports
        uses: actions/upload-artifact@v4
        with:
          name: code-quality-report-${{ github.run_id }}
          path: quality-reports/
          retention-days: 30

  security-tests:
    name: Security Testing
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install security tools
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety semgrep
          pip install -r requirements.txt

      - name: Run Bandit (Security Linter)
        run: |
          bandit -r tests/ *.py -f json -o security-reports/bandit-report.json || true
          bandit -r tests/ *.py || echo "::warning::Security issues found by Bandit"

      - name: Run Safety (Dependency Vulnerability Check)
        run: |
          mkdir -p security-reports
          safety check --json --output security-reports/safety-report.json || true
          safety check || echo "::warning::Vulnerable dependencies found"

      - name: Run Semgrep (SAST)
        run: |
          semgrep --config=auto tests/ *.py --json --output=security-reports/semgrep-report.json || true
          semgrep --config=auto tests/ *.py || echo "::warning::Security patterns found by Semgrep"

      - name: Check for Secrets in Code
        run: |
          # Check for potential secrets
          echo "ğŸ” Scanning for potential secrets..."
          grep -r -i "password\|secret\|key\|token" tests/ --exclude-dir=__pycache__ || echo "No obvious secrets found"
          
          # Check for hardcoded URLs and IPs
          grep -r -E "http://[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+" tests/ || echo "No hardcoded IPs found"

      - name: Generate Security Summary
        run: |
          mkdir -p security-reports
          echo "# Security Test Report" > security-reports/security-summary.md
          echo "Generated: $(date)" >> security-reports/security-summary.md
          echo "" >> security-reports/security-summary.md
          echo "## Tests Performed:" >> security-reports/security-summary.md
          echo "- Bandit: Static security analysis" >> security-reports/security-summary.md
          echo "- Safety: Dependency vulnerability scanning" >> security-reports/security-summary.md
          echo "- Semgrep: Static Application Security Testing (SAST)" >> security-reports/security-summary.md
          echo "- Secrets detection: Manual pattern matching" >> security-reports/security-summary.md

      - name: Upload Security Reports
        uses: actions/upload-artifact@v4
        with:
          name: security-reports-${{ github.run_id }}
          path: security-reports/
          retention-days: 30

  load-tests:
    name: Load & Performance Testing
    runs-on: ubuntu-latest
    # Run load tests automatically, but skip heavy load testing unless manually triggered
    if: ${{ !cancelled() }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install load testing tools
        run: |
          python -m pip install --upgrade pip
          pip install locust pytest-benchmark requests
          pip install -r requirements.txt

      - name: Create Load Test Scripts
        run: |
          mkdir -p load-tests
          
          # Create Locust load test for Firefly API
          cat > load-tests/firefly_load_test.py << 'EOF'
          from locust import HttpUser, task, between
          import os
          
          class FireflyAPIUser(HttpUser):
              wait_time = between(1, 3)
              
              def on_start(self):
                  self.headers = {
                      "Authorization": f"Bearer {os.getenv('API_TESTING_TOKEN', 'dummy_token')}",
                      "Content-Type": "application/json",
                      "Accept": "application/vnd.api+json"
                  }
              
              @task(3)
              def get_about(self):
                  with self.client.get("/api/v1/about", headers=self.headers, catch_response=True) as response:
                      if response.status_code == 200:
                          response.success()
                      elif response.status_code in [401, 403]:
                          response.success()  # Expected for load testing without real auth
                      else:
                          response.failure(f"Unexpected status: {response.status_code}")
              
              @task(2)
              def get_accounts(self):
                  with self.client.get("/api/v1/accounts", headers=self.headers, catch_response=True) as response:
                      if response.status_code in [200, 401, 403]:
                          response.success()
                      else:
                          response.failure(f"Unexpected status: {response.status_code}")
              
              @task(1)
              def get_transactions(self):
                  with self.client.get("/api/v1/transactions", headers=self.headers, catch_response=True) as response:
                      if response.status_code in [200, 401, 403]:
                          response.success()
                      else:
                          response.failure(f"Unexpected status: {response.status_code}")
          EOF

      - name: Run Performance Benchmarks (Unit Tests)
        run: |
          # Benchmark our unit tests for performance
          python -m pytest tests/test_ai_unit.py tests/test_webhook_unit.py -v --benchmark-only --benchmark-json=load-tests/unit-benchmark.json || true

      - name: Check Remote Firefly Availability for Load Testing
        id: load_test_check
        run: |
          if curl -f -s --max-time 5 "${{ env.FIREFLY_URL }}/api/v1/about" > /dev/null; then
            echo "available=true" >> $GITHUB_OUTPUT
            echo "âœ… Remote Firefly III available for load testing"
          else
            echo "available=false" >> $GITHUB_OUTPUT
            echo "âš ï¸ Remote Firefly III unavailable - simulating load tests"
          fi

      - name: Run Load Tests (if Firefly available)
        if: steps.load_test_check.outputs.available == 'true'
        env:
          API_TESTING_TOKEN: ${{ secrets.API_TESTING_TOKEN }}
        run: |
          cd load-tests
          # Light load test for automatic runs, heavier for manual triggers
          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.event.inputs.test_type }}" == "load" ]]; then
            echo "ğŸ”¥ Running full load test (manual trigger)"
            locust -f firefly_load_test.py --headless --users 20 --spawn-rate 5 --run-time 120s --host "${{ env.FIREFLY_URL }}" --html load-test-report.html
          else
            echo "âš¡ Running light load test (automatic)"
            locust -f firefly_load_test.py --headless --users 5 --spawn-rate 1 --run-time 30s --host "${{ env.FIREFLY_URL }}" --html load-test-report.html
          fi

      - name: Simulate Load Tests (if Firefly unavailable)
        if: steps.load_test_check.outputs.available == 'false'
        run: |
          echo "ğŸ”„ Simulating load test results..."
          mkdir -p load-tests
          cat > load-tests/simulated-load-report.html << 'EOF'
          <html><head><title>Simulated Load Test Report</title></head>
          <body>
          <h1>Load Test Report (Simulated)</h1>
          <p>Remote Firefly III instance was unavailable during test execution.</p>
          <p>This is a placeholder report. In a real scenario with available services:</p>
          <ul>
          <li>10 concurrent users</li>
          <li>60 second test duration</li>
          <li>API endpoints: /about, /accounts, /transactions</li>
          <li>Expected response times: < 500ms</li>
          </ul>
          </body></html>
          EOF

      - name: Generate Load Test Summary
        run: |
          mkdir -p load-tests
          echo "# Load Test Report" > load-tests/load-summary.md
          echo "Generated: $(date)" >> load-tests/load-summary.md
          echo "" >> load-tests/load-summary.md
          echo "## Test Configuration:" >> load-tests/load-summary.md
          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.event.inputs.test_type }}" == "load" ]]; then
            echo "- **Mode**: Full load test (manual trigger)" >> load-tests/load-summary.md
            echo "- **Users**: 20 concurrent" >> load-tests/load-summary.md
            echo "- **Duration**: 120 seconds" >> load-tests/load-summary.md
          else
            echo "- **Mode**: Light load test (automatic)" >> load-tests/load-summary.md
            echo "- **Users**: 5 concurrent" >> load-tests/load-summary.md
            echo "- **Duration**: 30 seconds" >> load-tests/load-summary.md
          fi
          echo "- **Target**: ${{ env.FIREFLY_URL }}" >> load-tests/load-summary.md
          echo "- **Endpoints tested**: /about, /accounts, /transactions" >> load-tests/load-summary.md

      - name: Upload Load Test Reports
        uses: actions/upload-artifact@v4
        with:
          name: load-test-reports-${{ github.run_id }}
          path: load-tests/
          retention-days: 30

  dependency-tests:
    name: Dependency Analysis
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependency tools
        run: |
          python -m pip install --upgrade pip
          pip install pip-audit pipdeptree pip-licenses
          pip install -r requirements.txt

      - name: Generate Dependency Tree
        run: |
          mkdir -p dependency-reports
          pipdeptree > dependency-reports/dependency-tree.txt
          pipdeptree --json > dependency-reports/dependency-tree.json

      - name: Check for Vulnerable Dependencies
        run: |
          pip-audit --format=json --output=dependency-reports/audit-report.json || echo "::warning::Vulnerable dependencies found"
          pip-audit || echo "::warning::Security audit completed with findings"

      - name: Generate License Report
        run: |
          pip-licenses --format=json --output-file=dependency-reports/licenses.json
          pip-licenses > dependency-reports/licenses.txt

      - name: Check for Outdated Packages
        run: |
          pip list --outdated > dependency-reports/outdated-packages.txt || true

      - name: Generate Dependency Summary
        run: |
          echo "# Dependency Analysis Report" > dependency-reports/dependency-summary.md
          echo "Generated: $(date)" >> dependency-reports/dependency-summary.md
          echo "" >> dependency-reports/dependency-summary.md
          echo "## Analysis Performed:" >> dependency-reports/dependency-summary.md
          echo "- Dependency tree analysis" >> dependency-reports/dependency-summary.md
          echo "- Security vulnerability scanning" >> dependency-reports/dependency-summary.md
          echo "- License compatibility check" >> dependency-reports/dependency-summary.md
          echo "- Outdated package detection" >> dependency-reports/dependency-summary.md

      - name: Upload Dependency Reports
        uses: actions/upload-artifact@v4
        with:
          name: dependency-reports-${{ github.run_id }}
          path: dependency-reports/
          retention-days: 30
          
  allure-report:
    name: Firefly III Test Suite - Generate Test Reports
    runs-on: ubuntu-latest
    needs: [unit-tests, api-tests, code-quality, security-tests, dependency-tests, load-tests]
    if: always()
    environment:
      name: github-pages
    permissions:
      contents: read
      pages: write
      id-token: write
      issues: write
      pull-requests: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download previous Allure history
        uses: actions/download-artifact@v4
        with:
          name: allure-history-firefly
          path: allure-results/history
        continue-on-error: true

      - name: Create initial Allure history if not exists
        run: |
          mkdir -p allure-results/history
          if [ ! -f allure-results/history/history.json ]; then
            echo '{"statistic": {"total": 0, "passed": 0, "failed": 0, "broken": 0, "skipped": 0}}' > allure-results/history/history.json
          fi

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: |
            allure-results-*-${{ github.run_id }}
            code-quality-report-${{ github.run_id }}
            security-reports-${{ github.run_id }}
            load-test-reports-${{ github.run_id }}
            dependency-reports-${{ github.run_id }}
          merge-multiple: true
          path: reports-downloaded

      - name: Merge Allure results
        run: |
          mkdir -p allure-results
          cp -r reports-downloaded/* allure-results/ 2>/dev/null || true
          cat > allure-results/environment.properties << EOF
          Test.Suite=Firefly III Test Suite
          Environment=${{ github.event.inputs.environment || 'staging' }}
          Python.Version=3.11
          GitHub.Run=${{ github.run_id }}
          GitHub.Workflow=${{ github.workflow }}
          Firefly.URL=${{ env.FIREFLY_URL }}
          Test.Types=Unit, API, Security, Load, Quality, Dependencies
          Timestamp=$(date -u +"%Y-%m-%d %H:%M:%S UTC")
          EOF

      - name: Generate Allure Report
        uses: simple-elf/allure-report-action@master
        with:
          allure_results: allure-results
          allure_report: allure-report-firefly
          keep_reports: 3

      - name: Create GitHub Pages Index
        run: |
          mkdir -p pages-content/test-reports
          cp -r allure-report-firefly/* pages-content/test-reports/
          cat > pages-content/index.html << 'EOF'
          <!DOCTYPE html>
          <html lang="en">
          <head>
              <meta charset="UTF-8">
              <meta name="viewport" content="width=device-width, initial-scale=1.0">
              <title>Firefly III Test Reports</title>
              <style>
                  body { font-family: Arial, sans-serif; margin: 40px; background: #f5f5f5; }
                  .container { max-width: 800px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
                  h1 { color: #333; text-align: center; margin-bottom: 30px; }
                  .report-link { display: block; padding: 15px; margin: 10px 0; background: #007bff; color: white; text-decoration: none; border-radius: 5px; text-align: center; transition: background 0.3s; }
                  .report-link:hover { background: #0056b3; }
                  .info { background: #e9ecef; padding: 15px; border-radius: 5px; margin: 20px 0; }
                  .timestamp { text-align: center; color: #666; margin-top: 20px; }
              </style>
          </head>
          <body>
              <div class="container">
                  <h1>ğŸ§ª Firefly III Test Reports</h1>
                  <div class="info">
                      <strong>Test Suite Coverage:</strong>
                      <ul>
                          <li>âœ… Unit Tests (AI Categorizer & Webhook Service)</li>
                          <li>ğŸ”Œ API Integration Tests</li>
                          <li>ğŸ›¡ï¸ Security Tests</li>
                          <li>ğŸ“Š Code Quality</li>
                          <li>âš¡ Load & Performance Tests</li>
                          <li>ğŸ“¦ Dependency Analysis</li>
                      </ul>
                  </div>
                  <a href="test-reports/" class="report-link">ğŸ“Š View Latest Test Report</a>
                  <div class="timestamp">
                      Last updated: $(date -u +"%Y-%m-%d %H:%M:%S UTC")<br>
                      GitHub Run: #${{ github.run_id }}
                  </div>
              </div>
          </body>
          </html>
          EOF

      - name: Setup Pages
        uses: actions/configure-pages@v4
        if: always()

      - name: Upload to GitHub Pages
        uses: actions/upload-pages-artifact@v3
        if: always()
        with:
          path: pages-content

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
        if: always()